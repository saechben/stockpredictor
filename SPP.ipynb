{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os \n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset using panda \n",
    "#this loads one specific csv file\n",
    "#dataset = pd.read_csv(r'C:\\Users\\benja\\OneDrive\\Dokumente\\Wichtiges\\Projects\\stock_price_predictor\\dataset\\archive\\Data\\ETFs\\aadr.us.txt')\n",
    "#dataset.head()\n",
    "#recursively load all csv files into a list\n",
    "#sets my working directory so the glob can fetch all the files\n",
    "os.chdir(r'C:\\Users\\Admin\\OneDrive\\Dokumente\\Wichtiges\\Projects\\stock_price_predictor\\dataset\\archive\\Data')\n",
    "file_list = glob.glob('**/*.*',recursive=True)\n",
    "\"\"\"\"\"\n",
    "dataset = pd.read_csv(file_list[0],delimiter=',',header=0)\n",
    "print(dataset.info)\n",
    "dataset2 = pd.read_csv(file_list[1],delimiter=',',header=0)\n",
    "print(dataset2.info)\n",
    "result = pd.concat([dataset.reset_index(drop=True),dataset2.reset_index(drop=True)])\n",
    "print(result)\n",
    "\"\"\"\n",
    "dataset = pd.DataFrame()\n",
    "#merges every dataframe in one big dataframe\n",
    "\"\"\"\"\"\n",
    "for file in file_list:\n",
    "    if(os.stat(file).st_size!=0):\n",
    "        temp = pd.read_csv(file,sep=',',header=0)\n",
    "        dataset = pd.concat([dataset.reset_index(drop=True),temp.reset_index(drop=True)])\n",
    "print(dataset)\n",
    "\"\"\"\n",
    "#merges every dataframe in one big dataframe and check for empty files\n",
    "for file in file_list:\n",
    "    if(os.stat(file).st_size!=0):\n",
    "        temp = pd.read_csv(file,sep=',',header=0)\n",
    "        dataset = pd.concat([dataset.reset_index(drop=True),temp.reset_index(drop=True)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#order the dataset by date --> don't know if this is necessary\n",
    "dataset = dataset.sort_values(by=['Date'])\n",
    "#droping the date column --> not needed\n",
    "dataset = dataset.drop(['Date'],axis=1)\n",
    "#rescale the data to -1-1\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "dataset = scaler.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the data\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training, test, validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for splitting the data since the predicted label is always the stock price for of the \"next day\"\n",
    "def split_data(data,look_back):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-look_back-1):\n",
    "        x.append(data[i:(i+look_back),:])\n",
    "        y.append(data[i+look_back,:])\n",
    "    return np.array(x),np.array(y)\n",
    "def split_sets(x,y,x_split):\n",
    "    x_train = x[:x_split]\n",
    "    y_train = y[:x_split]\n",
    "    x_test = x[x_split:]\n",
    "    y_test = y[x_split:]\n",
    "    return x_train,y_train,x_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = split_data(dataset,2)\n",
    "#define training split --> 80% training, 10% testing, 10% validation\n",
    "training_split = 0.8\n",
    "#get the index where the data should be splitted\n",
    "x_split = int(np.ceil(len(x)*training_split))\n",
    "#split the data into training and testing and validation sets\n",
    "x_train,y_train,x_test,y_test = split_sets(x,y,x_split)\n",
    "#split the testing data into testing and validation sets with 70% testing and 30% validation\n",
    "testing_split = 0.7\n",
    "x_split = int(np.ceil(len(x_test)*testing_split))\n",
    "x_test,y_test,x_val,y_val = split_sets(x_test,y_test,x_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 2, 50)             11400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 50)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 2, 50)             20200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2, 50)             0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 306       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,106\n",
      "Trainable params: 52,106\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50,return_sequences=True,input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "#dropout for regularization --> randomly drops 20% of the neurons\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "#output layer --> fully connected layer\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "#compiling the model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "#overfitting on one sample\n",
    "model.fit(x_train,y_train,validation_data=(x_val,y_val),batch_size=1,epochs=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('oldenvSPP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b291799f6606bb81e426db0302226367a5ab9cddbd0310f5985eed57b9a89cc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
