{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os \n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n",
      "empty file\n"
     ]
    }
   ],
   "source": [
    "#loading dataset using panda \n",
    "#this loads one specific csv file\n",
    "#dataset = pd.read_csv(r'C:\\Users\\benja\\OneDrive\\Dokumente\\Wichtiges\\Projects\\stock_price_predictor\\dataset\\archive\\Data\\ETFs\\aadr.us.txt')\n",
    "#dataset.head()\n",
    "#recursively load all csv files into a list\n",
    "#sets my working directory so the glob can fetch all the files\n",
    "os.chdir(r'C:\\Users\\Admin\\OneDrive\\Dokumente\\Wichtiges\\Projects\\stock_price_predictor\\dataset\\archive\\Data')\n",
    "file_list = glob.glob('**/*.*',recursive=True)\n",
    "dataset = pd.DataFrame()\n",
    "#merges every dataframe in one big dataframe and check for empty files\n",
    "for file in file_list:\n",
    "    try:\n",
    "        dataset = dataset.append(pd.read_csv(file))\n",
    "    except:\n",
    "        print(\"empty file\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12684\\3684617478.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#order the dataset by date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#only use the date and the adjusted close price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Adj Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#rescale the data to -1-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "\n",
    "#order the dataset by date\n",
    "dataset = dataset.sort_values(by=['Date'])\n",
    "#only use the date and the adjusted close price\n",
    "dataset = dataset[['Date','Adj Close']]\n",
    "#rescale the data to -1-1\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "dataset = scaler.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tensorboard to visualize the training process\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format('SPP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into training, test, validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for splitting the data since the predicted label is always the stock price for of the \"next day\"\n",
    "def split_data(data,look_back):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-look_back-1):\n",
    "        x.append(data[i:(i+look_back),:])\n",
    "        y.append(data[i+look_back,:])\n",
    "    return np.array(x),np.array(y)\n",
    "def split_sets(x,y,x_split):\n",
    "    x_train = x[:x_split]\n",
    "    y_train = y[:x_split]\n",
    "    x_test = x[x_split:]\n",
    "    y_test = y[x_split:]\n",
    "    return x_train,y_train,x_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = split_data(dataset,2)\n",
    "#define training split --> 80% training 20% testing\n",
    "training_split = 0.8\n",
    "#get the index where the data should be splitted\n",
    "x_split = int(np.ceil(len(x)*training_split))\n",
    "#split the data into training and testing data\n",
    "x_train,y_train,x_test,y_test = split_sets(x,y,x_split)\n",
    "#split the training data into training and validation data with a 80/20 split\n",
    "x_train,x_val,y_train,y_val = split_sets(x_train,y_train,int(np.ceil(len(x_train)*0.8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.         -1.         -1.         -0.9978747  -1.        ]\n",
      " [-0.99999999 -0.99999999 -0.99999999 -0.99999999 -0.9996146  -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#visualize the data\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 2, 50)             11400     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 2, 50)             0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 2, 50)             20200     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 2, 50)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 51,902\n",
      "Trainable params: 51,902\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#defining the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50,return_sequences=True,input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "#dropout for regularization --> randomly drops 50% of the neurons\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units = 50,return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.5))\n",
    "#output layer --> fully connected layer\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "#compiling the model\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "#for description of the model --> none = dimnension is variable\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 12 into shape (1,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12684\\1888216239.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#overfitting model on one sample of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 12 into shape (1,2)"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "#overfitting model on one sample of the data\n",
    "model.fit(x_train[0].reshape(1,x_train.shape[1],x_train.shape[2]),y_train[0].reshape(1,y_train.shape[1]),epochs=100,batch_size=1,verbose=1,callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436331/436331 [==============================] - 2631s 6ms/step - loss: 3.4827e-05 - val_loss: 1.2068e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20cf7903348>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model on the training set\n",
    "model.fit(x_train,y_train,validation_data=(x_val,y_val),batch_size=32,epochs=1,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model --> current model SPP.h5\n",
    "model.save('SPP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model = tf.keras.models.load_model('SPP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76358/76358 [==============================] - 172s 2ms/step\n",
      "146690.4358881029\n"
     ]
    }
   ],
   "source": [
    "#testing the model\n",
    "#predicting the stock price for the next day\n",
    "y_pred = model.predict(x_test)\n",
    "#rescale the data back to the original scale\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "#calculate the error\n",
    "error = np.mean(np.abs(y_pred-y_test))\n",
    "#visualize the results\n",
    "plt.plot(y_pred)\n",
    "plt.plot(y_test)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('oldenvSPP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b291799f6606bb81e426db0302226367a5ab9cddbd0310f5985eed57b9a89cc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
